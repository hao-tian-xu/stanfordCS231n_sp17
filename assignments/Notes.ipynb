{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning / Linear Algebra / Numpy / PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 (Euclidean) Distance / Broadcast / Dot Product\n",
    "$$\n",
    "(a-b)^2 = a^2-2ab+b^2\n",
    "$$ \n",
    "(Matrix Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square 1: \n",
      " [[ 5]\n",
      " [25]]\n",
      "Square 2: \n",
      " [  5  25  61 113]\n",
      "Sum of squares: \n",
      " [[ 10  30  66 118]\n",
      " [ 30  50  86 138]]\n",
      "Multipliction: \n",
      " [[ 5 11 17 23]\n",
      " [11 25 39 53]]\n",
      "Dot product: \n",
      " [[ 5 11 17 23]\n",
      " [11 25 39 53]]\n",
      "L2 distance: \n",
      " [[0.         2.82842712 5.65685425 8.48528137]\n",
      " [2.82842712 0.         2.82842712 5.65685425]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_test = np.array([[1, 2], [3, 4]])\n",
    "X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "# reshape to broadcast each test example for all training examples\n",
    "sqr_1 = np.reshape(np.sum(X_test ** 2, axis=1), (X_test.shape[0], 1))\n",
    "sqr_2 = np.sum(X_train ** 2, axis=1)\n",
    "print('Square 1: \\n', sqr_1)\n",
    "print('Square 2: \\n', sqr_2)\n",
    "print('Sum of squares: \\n', sqr_1 + sqr_2)\n",
    "\n",
    "# dot product of the test set and the training set's transposed matrix\n",
    "multi = np.matmul(X_test, X_train.T)\n",
    "_multi = np.dot(X_test, X_train.T)\n",
    "l2_dist = np.sqrt(sqr_1 - 2 * multi + sqr_2)\n",
    "print('Multipliction: \\n', multi)\n",
    "print('Dot product: \\n', _multi)\n",
    "print('L2 distance: \\n', l2_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier / Hinge Loss\n",
    "The example of the SVM loss function for a single datapoint:\n",
    "$$\n",
    "L_{i}=\\sum_{j \\neq y_{i}}\\left[\\max \\left(0, w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\\Delta\\right)\\right]\n",
    "$$\n",
    "Taking the gradient with respect to $w_{y_i}$ we obtain:\n",
    "$$\n",
    "\\nabla_{w_{y_{i}}} L_{i}=-\\left(\\sum_{j \\neq y_{i}} \\mathbb{1}\\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\\Delta>0\\right)\\right) x_{i}\n",
    "$$\n",
    "(when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient.)\n",
    "\n",
    "For the other rows where $j\\neq y_i$ the gradient is:\n",
    "$$\n",
    "\\nabla_{w_{j}} L_{i}=\\mathbb{1}\\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\\Delta>0\\right) x_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      " 2.0069131264939335\n",
      "\n",
      "Gradient: \n",
      " [[ 1.75 -2.    0.25]\n",
      " [ 2.   -2.5   0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "regularization = 5e-7\n",
    "\n",
    "X: ndarray = np.array([[1,2], [3, 4], [5, 6], [7, 8]])\n",
    "y: ndarray = np.array([1, 0, 2, 1])\n",
    "\n",
    "num_train, dim = X.shape\n",
    "num_classes = np.max(y) + 1\n",
    "\n",
    "W: ndarray = 0.001 * np.random.randn(dim, num_classes)\n",
    "dW: ndarray = np.zeros(W.shape)\n",
    "\n",
    "## Hinge Loss\n",
    "# linear classifier\n",
    "scores: ndarray = X.dot(W)\n",
    "# scores of correct class\n",
    "correct_class_score: ndarray = scores[np.arange(num_train), y]\n",
    "# reshape the correct_class_score in order to broadcast to the shape of scores\n",
    "reshaped_correct_class_score: ndarray = correct_class_score.reshape((num_train, 1))\n",
    "# margin\n",
    "margin: ndarray = np.maximum(0, scores - reshaped_correct_class_score + 1)\n",
    "margin[np.arange(num_train), y] = 0     # correct class margin\n",
    "# data loss and regularization loss\n",
    "loss = np.sum(margin) / num_train + regularization * np.sum(W * W)\n",
    "# print('Scores: \\n', scores)\n",
    "# print('Correct_class_scores: \\n', correct_class_score)\n",
    "# print('Reshaped_correct_class_scores: \\n', reshaped_correct_class_score)\n",
    "# print('Margin: \\n', margin)\n",
    "print('Loss: \\n', loss)\n",
    "print()\n",
    "\n",
    "## Gradient\n",
    "# a mask to add certain X_i to certain dW[:, j]\n",
    "mask: ndarray = np.zeros(margin.shape)\n",
    "# gradient of incorrect classes: X_i when margin > 0 \n",
    "mask[margin > 0] = 1\n",
    "# gradient of correct classes: -k * X_i where k is the number of classes with margin > 0\n",
    "mask[np.arange(num_train), y] -= np.sum(mask, axis=1)\n",
    "# add k * X_i to dW[:, j] where k = mask_ij\n",
    "data_loss_gradient: ndarray = X.T.dot(mask) / num_train\n",
    "# data loss gradient and regularization loss gradient\n",
    "dW = data_loss_gradient + 2 * regularization * W\n",
    "# print('Mask: \\n', mask)\n",
    "print('Gradient: \\n', dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier / Cross-entropy Loss\n",
    "Scores: \n",
    "$$\n",
    "f = WX\n",
    "$$\n",
    "\n",
    "Softmax Function:\n",
    "$$\n",
    "p_i = \\frac{e^{f_i}}{\\sum_{j} e^{f_{j}}}\n",
    "$$\n",
    "\n",
    "Loss Function:\n",
    "$$\n",
    "L_i = -\\log p_{y_i}\n",
    "$$\n",
    "\n",
    "Gradient Function:\n",
    "$$\n",
    "\\nabla_{w_j}L_i = \\sum_i (p_i-\\mathbb{1}(i = y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0971536661606218 \n",
      " [[ 0.57449087 -0.65888945  0.08439858]\n",
      " [ 0.65602617 -0.82397117  0.16794501]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "regularization = 5e-7\n",
    "\n",
    "X: ndarray = np.array([[1,2], [3, 4], [5, 6], [7, 8]])\n",
    "y: ndarray = np.array([1, 0, 2, 1])\n",
    "\n",
    "num_train, dim = X.shape\n",
    "num_classes = np.max(y) + 1\n",
    "\n",
    "W: ndarray = 0.001 * np.random.randn(dim, num_classes)\n",
    "dW: ndarray = np.zeros(W.shape)\n",
    "\n",
    "scores = X.dot(W)\n",
    "# memo: numeric stability to avoid dividing large numbers due to the exponential\n",
    "scores -= np.max(scores, axis=1, keepdims=True)\n",
    "\n",
    "# memo: softmax\n",
    "probs = np.exp(scores)\n",
    "sum_probs = np.sum(probs, axis=1, keepdims=True)\n",
    "softmax = np.divide(probs, sum_probs)\n",
    "\n",
    "# memo: data loss and regularization loss\n",
    "loss = -np.sum(np.log(softmax[np.arange(num_train), y])) / num_train \\\n",
    "       + regularization * np.sum(W * W)\n",
    "\n",
    "# memo: mask for gradient calculation\n",
    "mask = np.copy(softmax)\n",
    "mask[np.arange(num_train), y] -= 1\n",
    "\n",
    "# memo: data gradient and regularization gradient\n",
    "dW = X.T.dot(mask) / num_train + 2 * regularization * W\n",
    "\n",
    "print(loss, '\\n', dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Linear Algebra**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for vectorized operations\n",
    "##### Matrix-Matrix multiply gradient\n",
    "Tip: use dimension analysis! Note that you do not need to remember the expressions for `dX` and `dW` because they are easy to re-derive based on dimensions. For instance, we know that the gradient on the weights `dX` must be of the same size as `X` after it is computed, and that it must depend on matrix multiplication of `W` and `dS` (as is the case when both `W,X` are single numbers and not matrices). There is always exactly one way of achieving this so that the dimensions work out. For example, `W` is of size [10 x 3] and `dS` of size [5 x 3], so if we want `dX` and `X` has shape [5 x 10], then the only way of achieving this is with `dS.dot(W.T)`, as shown above.\n",
    "\n",
    "++Note: above tips only work on Loss Functions (scalar output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# forward pass\n",
    "X = np.random.randn(5, 10)  # (N, D)\n",
    "W = np.random.randn(10, 3)  # (D, C)\n",
    "S = X.dot(W)                # (N, C)\n",
    "\n",
    "# now suppose we had the gradient on S from above in the circuit\n",
    "# dS: dL/dS, dX: dL/dX, dW: dL/dW\n",
    "dS = np.random.randn(*S.shape) # same shape as S\n",
    "dX = dS.dot(W.T) #.T gives the transpose of the matrix\n",
    "dW = X.T.dot(dS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max vs. Maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "\n",
      "[[3 4]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2], [5, 6]])\n",
    "b = np.array([3, 4])\n",
    "\n",
    "print(np.max(a))\n",
    "print()\n",
    "print(np.maximum(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask (Boolean array indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False]\n",
      " [ True  True]\n",
      " [ True  True]]\n",
      "\n",
      "[[1 2]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2], [3, 4], [5, 6]])\n",
    "bool_ind = a > 2\n",
    "print(bool_ind)\n",
    "print()\n",
    "a[a>2] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing vs. Indexing\n",
    "To index into a matrix, use `np.arrange()` instead of `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2]\n",
      " [4 3 4]\n",
      " [6 5 6]]\n",
      "\n",
      "[2 3 6]\n",
      "\n",
      "[2 3 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2], [3, 4], [5, 6]])\n",
    "\n",
    "b = a[:, [1, 0, 1]]\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "c = a[np.arange(3), [1, 0, 1]]\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "d = a[[0, 1, 2], [1, 0, 1]]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2]]\n",
      "\n",
      " [[3 4]]\n",
      "\n",
      " [[1 2]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[0], [1], [0]])\n",
    "\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "\n",
    "x = x.reshape((2, 1, 3, 4, 4))\n",
    "\n",
    "print((x * w).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification to reshaped matrix\n",
    "It works when the matrix is not a slice of another matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [1. 1.]]]]\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "[[[[2. 1.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[1. 1.]\n",
      "   [2. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 2.]\n",
      "   [1. 1.]]\n",
      "\n",
      "  [[2. 1.]\n",
      "   [1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1.]\n",
      "   [2. 1.]]\n",
      "\n",
      "  [[1. 2.]\n",
      "   [1. 1.]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = np.ones((3, 2, 4, 4))\n",
    "\n",
    "# c is a slice of b\n",
    "c = b[:, :, 0:2, 0:2]\n",
    "print(type(c))\n",
    "c.reshape((6, 4))[np.arange(6), np.array([0, 2, 1, 0, 2, 1])] += 1\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "d = np.ones((3, 2, 2, 2))\n",
    "print(type(d))\n",
    "d.reshape((6, 4))[np.arange(6), np.array([0, 2, 1, 0, 2, 1])] += 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `view()`, `reshape()`, `permute()`, `transpose()`, and `contiguous()`\n",
    "\n",
    "- There are a few operations on Tensors in PyTorch that do not change the contents of a tensor, but change the way the data is organized. These operations include:\n",
    "    \n",
    "    `narrow()`, `view()`, `expand()` and `transpose()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6d501d8de5db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m                 \u001b[0;31m# underlying data is still in shape (1, 2, 3, 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(1, 2, 3, 4)\n",
    "\n",
    "b = a.permute(0, 2, 1, 3).view(1, 3, 8)                 # underlying data is still in shape (1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.permute(0, 2, 1, 3).reshape(1, 3, 8)              # use reshape instead\n",
    "c = a.permute(0, 2, 1, 3).contiguous().view(1, 3, 8)    # or make a contiguous copy before view\n",
    "\n",
    "c = a.transpose(1, 2).contiguous().view(1, 3, 8)        # transpose is like permute but can only permute 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.arange(0, 10, 2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
