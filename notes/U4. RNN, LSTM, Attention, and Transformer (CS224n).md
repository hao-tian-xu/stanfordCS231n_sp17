# **<u>Lecture 6 - Simple and LSTM RNNs</u>**

## **Vanishing and Exploding Gradients**

### Vanishing gradients

- Recall:
  $$
  \boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)
  $$

- What if $\sigma$ were the identity function, $\sigma(x)=x$ ?
  $$
  \begin{array}{llr}
  \frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}} &=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right) \boldsymbol{W}_{h} & \text{(chain rule)} \\
  &=\boldsymbol{I} \boldsymbol{W}_{h}=\boldsymbol{W}_{h}
  \end{array}
  $$

- Consider the gradient of the loss $J^{(i)}(\theta)$ on step $i$, with respect to the hidden state $\boldsymbol{h}^{(j)}$ on some previous step $j$. Let $\ell=i-j$
  $$
  \begin{array}{llr}
  \frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}} & =\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} \prod_{j<t \leq i} \frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}} & \text { (chain rule) } \\
  & =\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} \prod_{j<t \leq i} \boldsymbol{W}_{h}=\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} \boldsymbol{W}_{h}^{\ell} \quad \text { (value of } \frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}} \text { ) }
  \end{array}
  $$
  - If $W_h$ is “small”, then this term gets exponentially problematic as $\ell$ becomes large
- Consider if the eigenvalues of $W_{h}$ are all less than 1 :
  $$
  \begin{aligned}
  &\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}<1 \\
  &\boldsymbol{q}_{1}, \boldsymbol{q}_{2}, \ldots, \boldsymbol{q}_{n} \text { (eigenvectors) }
  \end{aligned}
  $$
- We can write $\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} W_{h}^{\ell}$ using the eigenvectors of $\boldsymbol{W}_{h}$ as a basis:
  $$
  \frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}} \boldsymbol{W}_{h}^{\ell}=\sum_{i=1}^{n} c_{i} \lambda_{i}^{\ell} \boldsymbol{q}_{i} \approx \mathbf{0}(\text { for large } \ell)
  $$
  - $\lambda_i^\ell$ Approaches 0 as $\ell$ grows, so gradient vanishes
- What about nonlinear activations $\sigma$ (i.e., what we use?)

  - Pretty much the same thing, except the proof requires $\lambda_{i}<\gamma$ for some $\gamma$ dependent on dimensionality and $\sigma$

- <u>Source:</u>
  - “On the difficulty of training recurrent neural networks”, Pascanu et al, 2013. http://proceedings.mlr.press/v28/pascanu13.pdf
  - (and supplemental materials), at http://proceedings.mlr.press/v28/pascanu13-supp.pdf

### Why is vanishing gradient a problem?

- Gradient signal from far away is lost because it’s much smaller than gradient signal from close-by. 
  - So, model weights are updated only with respect to near effects, not long-term effects.
- e.g. LM task: When she tried to print her **tickets**, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ____

### Exploding gradients

- Problem: If the gradient becomes too big, then the SGD update step becomes too big

- Solution: gradient clipping

  - if the norm of the gradient is greater than some threshold, scale it down before applying SGD update
    $$
    \begin{aligned}
    &\hat{\mathbf{g}} \leftarrow \frac{\partial \mathcal{E}}{\partial \theta} \\
    &\text {if }\|\hat{\mathbf{g}}\| \geq \text { threshold then } \\
    &\quad \hat{\mathbf{g}} \leftarrow \frac{t h r e s h o l d}{\|\hat{\mathbf{g}}\|} \hat{\mathbf{g}} \\
    &\text {end if }
    \end{aligned}
    $$
    


### How to fix the vanishing gradient problem?

- The main problem is that it’s too difficult for the RNN to learn to preserve information over many timesteps.
- In a vanilla RNN, the hidden state is constantly being rewritten
- How about a RNN with separate memory?

## **Long Short-Term Memory RNNs (LSTMs)**

<img src="image.assets/LSTM3-chain-8191127.png" alt="LSTM3-chain" style="zoom: 20%;" />

<img src="image.assets/LSTM3-C-line-8191154.png" alt="LSTM3-C-line" style="zoom: 25%;" />

- “Long short-term memory”, Hochreiter and Schmidhuber, 1997. https://www.bioinf.jku.at/publications/older/2604.pdf

- “Learning to Forget: Continual Prediction with LSTM”, Gers, Schmidhuber, and Cummins, 2000. https://dl.acm.org/doi/10.1162/089976600300015015

- On step $t$, there is a hidden state $\boldsymbol{h}^{(t)}$ and a cell state $\boldsymbol{c}^{(t)}$

  - Both are vectors length $n$
  - The cell stores long-term information
  - The LSTM can read, erase, and write information from the cell
    - The cell becomes conceptually rather like RAM in a computer

  $$
  \begin{array}{llll}
  \text{Forget gate: }&\boldsymbol{f}^{(t)} &=&\sigma\left(\boldsymbol{W}_{f} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{f} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{f}\right) \\
  \text{Input gate: } & \boldsymbol{i}^{(t)} &=&\sigma\left(\boldsymbol{W}_{i} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{i} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{i}\right) \\
  \text{Output gate: } & \boldsymbol{o}^{(t)} &=&\sigma\left(\boldsymbol{W}_{o} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{o} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{o}\right) \\
  \text{New cell content:: } & \tilde{\boldsymbol{c}}^{(t)} &=&\tanh \left(\boldsymbol{W}_{c} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{c} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{c}\right) \\
  \text{Cell state: } & \boldsymbol{c}^{(t)} &=&\boldsymbol{f}^{(t)} \odot \boldsymbol{c}^{(t-1)}+\boldsymbol{i}^{(t)} \odot \tilde{\boldsymbol{c}}^{(t)} \\
  \text{Hidden state: } & \boldsymbol{h}^{(t)} &=&\boldsymbol{o}^{(t)} \odot \tanh \boldsymbol{c}^{(t)}
  \end{array}
  $$

### How does LSTM solve vanishing gradients?

- The LSTM architecture makes it easier for the RNN to preserve information over many timesteps
  - e.g., if the forget gate is set to 1 for a cell dimension and the input gate set to 0, then the information of that cell is preserved indefinitely.
  - In practice, you get about 100 timesteps rather than about 7

### Is vanishing/exploding gradient just a RNN problem?

- No! It can be a problem for all neural architectures (including feed-forward and convolutional), especially very deep ones.
  - Due to chain rule / choice of nonlinearity function, gradient can become vanishingly small as it backpropagates
  - Thus, lower layers are learned very slowly (hard to train)
- Solution: lots of new deep feedforward/convolutional architectures that add more direct connections (thus allowing the gradient to flow) 
- Conclusion: Though vanishing/exploding gradients are a general problem, RNNs are particularly unstable due to the repeated multiplication by the same weight matrix [Bengio et al, 1994]

## **Bidirectional and Multi-layer RNNS**

- todo

## *Thinking*

- Somehow similar to ResNet, bringing the previous information directly into the current unit

- The experience of architecture

- - Although the overall architecture is still a complex function, the association between architecture and intuition makes training very simple. Here the proportion of previous information preserved, the proportion of new information incorporated, and the proportion of information passed forward.
  - In addition, architecture is also associated with machine learning problems. In this case, the flow of gradients/the vanishing gradients.



- 某种程度上类似于ResNet，把之前的信息直接带入当前单元
- 对于结构的经验：
  - 虽然大结构上还是一个复杂的函数，但结构和直觉的关联使得训练变得十分简单，这里是保存之前信息的比例、纳入新信息的比例、信息向前传递的比例
  - 除了结构和直觉的关联外，结构也和机器学习的问题相关联，这里是梯度的流动/梯度消失的问题

# **<u>Lecture 7 - Translation, Seq2Seq, Attention</u>**

### Section 1: Pre-Neural Machine Translation

- 1990s-2010s (1997-2013): Statistical Machine Translation (SMT)
  
  - Core idea: Learn a probabilistic model from data
  
  - e.g. We want to find best English sentence $y$, given French sentence $x$
    $$
    \operatorname{argmax}_{y} P(y \mid x) = \operatorname{argmax}_{y} \underbrace{P(x \mid y)}_\text{ Translation Model }\underbrace{P(y)}_\text{ Language Model }
    $$
  
  - Learning alignment for SMT
  
  - Decoding for SMT
  
- SMT was a huge research field 

- The best systems were extremely complex 

  - Systems had many separately-designed <u>subcomponents</u> 
  - Lots of <u>feature engineering</u> 

## **Section 2: Neural Machine Translation (NMT)**

- a single end-to-end neural network

### Sequence-to-sequence (seq2seq)

- The neural network architecture is called a sequence-to-sequence model (aka seq2seq) and it involves two RNNs

  - ​	<img src="image.assets/Screen Shot 2022-03-16 at 17.45.46.png" alt="Screen Shot 2022-03-16 at 17.45.46" style="zoom:20%;" />

  - Seq2seq is versatile: Summarization, Dialogue, Parsing, Code generation

  - NMT directly calculates $P(y \mid x)$ :
    $$
    P(y \mid x)=P\left(y_{1} \mid x\right) P\left(y_{2} \mid y_{1}, x\right) P\left(y_{3} \mid y_{1}, y_{2}, x\right) \ldots \underbrace{P\left(y_{T} \mid y_{1}, \ldots, y_{T-1}, x\right)}_{\begin{align}\text{Probability of next target word, given} \\ \text{target words so far and source sentence }x \end{align}}
    $$

  - Seq2seq is optimized as a single system. Backpropagation operates “end-to-end”.

### Multi-layer RNNs

- This allows the network to compute more complex representations 
  - ​	<img src="image.assets/Screen Shot 2022-03-16 at 17.58.09.png" alt="Screen Shot 2022-03-16 at 17.58.09" style="zoom:20%;" />
  - The lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features. 
  - In practic, for example: In a 2017 paper, Britz et al. find that for Neural Machine Translation, 2 to 4 layers is best for the encoder RNN, and 4 layers is best for the decoder RNN

### Beam search decoding

- Greedy / Brute-force
  - Greedy decoding
    - Take argmax on each step of the decoder
    - Problem: has no way to undo decisions
  - Exhaustive search decoding
    - We could try computing all possible sequences $y$
    - Problem: This $O(V^T)$ complexity is far too expensive
- Beam search decoding
  - Core idea: On each step of decoder, keep track of the $k$ most probable partial translations (which we call hypotheses)
- Finishing up
  - Usually we continue beam search until: 
    - We reach timestep T (where T is some pre-defined cutoff), or 
    - We have at least n completed hypotheses (where n is pre-defined cutoff)
  - Problem: longer hypotheses have lower scores
    - Fix: Normalize by length

### Evaluation

- BLEU (BiLingual Evaluation Understudy)
  - BLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score

- BLEU is useful but imperfect 
  - There are many valid ways to translate a sentence 
  - So a good translation can get a poor BLEU score because it has low n-gram overlap with the human translation


### Difficulties

- Out-of-vocabulary words 
- Domain mismatch between train and test data 
- Maintaining context over longer text 
- Low-resource language pairs 
- Failures to accurately capture sentence meaning 
- Pronoun (or zero pronoun) resolution errors 
- Morphological agreement errors

## **Section 3: Attention**

### Sequence-to-sequence: the bottleneck problem

- Information bottleneck:
  - Last encoding of the source sentence needs to capture all information about the source sentence. 

### Attention

- Core idea: on each step of the decoder, <u>use direct connection to the encoder to focus on a particular part of the source sequence</u>
  - ​	<img src="image.assets/Screen Shot 2022-03-16 at 18.09.03.png" alt="Screen Shot 2022-03-16 at 18.09.03" style="zoom:20%;" /></u>

### In Equation

- We have encoder hidden states $h_{1}, \ldots, h_{N} \in \mathbb{R}^{h}$
- On timestep $t$, we have decoder hidden state $s_{t} \in \mathbb{R}^{h}$
- We get the attention scores $e^{t}$ for this step:
$$
\boldsymbol{e}^{t}=\left[\boldsymbol{s}_{t}^{T} \boldsymbol{h}_{1}, \ldots, \boldsymbol{s}_{t}^{T} \boldsymbol{h}_{N}\right] \in \mathbb{R}^{N}
$$
- We take softmax to get the attention distribution $\alpha^{t}$ for this step (this is a probability distribution and sums to 1 )
$$
\alpha^{t}=\operatorname{softmax}\left(\boldsymbol{e}^{t}\right) \in \mathbb{R}^{N}
$$
- We use $\alpha^{t}$ to take a weighted sum of the encoder hidden states to get the attention output $\boldsymbol{a}_{t}$
$$
\boldsymbol{a}_{t}=\sum_{i=1}^{N} \alpha_{i}^{t} \boldsymbol{h}_{i} \in \mathbb{R}^{h}
$$
- Finally we concatenate the attention output $a_{t}$ with the decoder hidden state $s_{t}$ and proceed as in the non-attention seq2seq model
$$
\left[\boldsymbol{a}_{t} ; \boldsymbol{s}_{t}\right] \in \mathbb{R}^{2 h}
$$

### Attention is great

- Attention solves the bottleneck problem
- Attention helps with vanishing gradient problem
- Attention provides some interpretability
  - We get (soft) alignment for free

### Attention is a general Deep Learning technique

- More general definition of attention: 
  - Given a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query
- Intuition: 
  - The weighted sum is a selective summary of the information contained in the values, where the query determines which values to focus on. 
  - Attention is a way to obtain a fixed-size representation of an arbitrary set of representations (the values), dependent on some other representation (the query).

### Attention Variants

- There are several ways you can compute $\boldsymbol{e} \in \mathbb{R}^{N}$ from $\boldsymbol{h}_{1}, \ldots, \boldsymbol{h}_{N} \in \mathbb{R}^{d_{1}}$ and $s \in \mathbb{R}^{d_{2}}$ :
  - Basic dot-product attention: $\boldsymbol{e}_{i}=\boldsymbol{s}^{T} \boldsymbol{h}_{i} \in \mathbb{R}$
    - Note: this assumes $d_{1}=d_{2}$
    - This is the version we saw earlier
  - Multiplicative attention: $\boldsymbol{e}_{i}=\boldsymbol{s}^{T} \boldsymbol{W} \boldsymbol{h}_{i} \in \mathbb{R}$
    - Where $W \in \mathbb{R}^{d_{2} \times d_{1}}$ is a weight matrix
  - Additive attention: $\boldsymbol{e}_{i}=\boldsymbol{v}^{T} \tanh \left(\boldsymbol{W}_{1} \boldsymbol{h}_{i}+\boldsymbol{W}_{2} \boldsymbol{s}\right) \in \mathbb{R}$
    - Where $\boldsymbol{W}_{1} \in \mathbb{R}^{d_{3} \times d_{1}}, \boldsymbol{W}_{2} \in \mathbb{R}^{d_{3} \times d_{2}}$ are weight matrices and $\boldsymbol{v} \in \mathbb{R}^{d_{3}}$ is a weight vector.
    - $d_{3}$ (the attention dimensionality) is a hyperparameter

## *Thinking*

- The architecture of neural networks in machine translation

- The architecture of seq2seq:

- - RNN: encoder, decoder
  - Flexible decoding by beam search without an exponential increase in complexity

- Attention:

- - The weighted average of information in the encoder in each unit of the decoder as part of the input (solves the bottleneck of information transmission in seq2seq)



- 神经网络在机器翻译中的结构（architecture）
  - seq2seq的结构：
    - RNN：encoder，decoder
    - 通过束搜索进行灵活解码，又不使复杂度指数级增加
  - Attention：
    - 将encoder中的信息在decoder的每个单元中加权平均作为输入的一部分（解决seq2seq中信息传输的瓶颈）

# **<u>Lecture 9 - Self- Attention and Transformers</u>**

### Issues with recurrent models

- Linear interaction distance
  - RNNs take $O(\text{sequence length})$ steps for distant word pairs to interact, which means
    - Hard to learn long-distance dependencies (because gradient problems)
    - linear order isn’t the right way to think about sentences…
- Lack of parallelizability
  - Forward and backward passes have $O(\text{sequence length})$ unparallelizable operations
    - Inhibits training on very large datasets!

## **Self-Attention**

### Attention

- Number of unparallelizable operations does not increase sequence length. 
- Maximum interaction distance: $O(1)$, since all words interact at every layer!

### Self-attention

- In self-attention, the queries, keys, and values are drawn from the same source.

  - The (dot product) self-attention operation is as follows:
    $$
    \underbrace{e_{i j}=q_{i}^{\top} k_{j}}_
    {\begin{array}{cc}
    \text { Compute key- } \\
    \text { query affinities }
    \end{array}} \qquad
    
    \underbrace{\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{j^{\prime}} \exp \left(e_{i j^{\prime}}\right)}}_
    {\begin{array}{c}
    \text { Compute attention } \\
    \text { weights from affinities } \\
    \text { (softmax) }
    \end{array}} \qquad
    
    \underbrace{\text {output}_{i}=\sum_{j} \alpha_{i j} v_{j}}_
    {\begin{array}{cc}
    \text { Compute outputs as  } \\
    \text { weighted sum of values }
    \end{array}}
    $$

### Issue: Doesn’t have an inherent notion of order

- Consider representing each sequence index as a vector 
  $$
  p_{i} \in \mathbb{R}^{d} \text {, for } i \in\{1,2, \ldots, T\} \text { are position vectors }
  $$
- Add the $p_i$ to our inputs
- $p_i$
  - Sinusoidal position representations
    - Pros: 
      - Periodicity indicates that maybe “absolute position” isn’t as important 
      - Maybe can extrapolate to longer sequences as periods restart
    - Cons: 
      - Not learnable; also the extrapolation doesn’t really work
  - Position representation vectors learned from scratch
    - Pros: 
      - Flexibility: each position gets to be learned to fit the data 
    - Cons: 
      - Definitely can’t extrapolate to indices outside $1, … , T$
  - Sometimes people try more flexible representations of position: 
    - Relative linear position attention [Shaw et al., 2018] 
    - Dependency syntax-based position [Wang et al., 2019]

### Issue: No nonlinearities

- Easy fix: add a feed-forward network to post-process each output vector
  $$
  \begin{aligned}
  m_{i} &=M L P\left(\text { output }_{i}\right) \\
  &=W_{2} * \operatorname{ReLU}\left(W_{1} \times \text { output }_{i}+b_{1}\right)+b_{2}
  \end{aligned}
  $$
  

### Issue: Need to ensure we don’t “look at the future”

- ​	<img src="image.assets/Screen Shot 2022-03-18 at 15.58.35.png" alt="Screen Shot 2022-03-18 at 15.58.35" style="zoom:20%;" />
  
- To enable parallelization, we mask out attention to future words by setting attention scores to $-\infty$
  $$
  e_{i j}=\left\{\begin{array}{c}
  q_{i}^{\top} k_{j}, j<i \\
  -\infty, j \geq i
  \end{array}\right.
  $$

## **Transformer**

### Model

- ​	<img src="image.assets/Screen Shot 2022-03-16 at 18.41.18.png" alt="Screen Shot 2022-03-16 at 18.41.18" style="zoom:20%;" />
- Key-query-value attention
  - Let $x_{1}, \ldots, x_{T}$ be input vectors to the Transformer encoder; $x_{i} \in \mathbb{R}^{d}$
  - Then keys, queries, values are:
    - $k_{i}=K x_{i}$, where $K \in \mathbb{R}^{d \times d}$ is the key matrix.
    - $q_{i}=Q x_{i}$, where $Q \in \mathbb{R}^{d \times d}$ is the query matrix.
    - $v_{i}=V x_{i}$, where $V \in \mathbb{R}^{d \times d}$ is the value matrix.
  - In matrices:
    - Let $X=\left[x_{1} ; \ldots ; x_{T}\right] \in \mathbb{R}^{T \times d}$ be the concatenation of input vectors.
    - First, note that $X K \in \mathbb{R}^{T \times d}, X Q \in \mathbb{R}^{T \times d}, X V \in \mathbb{R}^{T \times d}$.
    - The output is defined as output $=\operatorname{softmax}\left(X Q(X K)^{\top}\right) \times X V$.
- Multi-headed attention
  - For word $i$, self-attention "looks" where $x_{i}^{\top} Q^{\top} K x_{j}$ is high, but maybe we want to focus on different $j$ for different reasons?
    - Let, $Q_{\ell}, K_{\ell}, V_{\ell} \in \mathbb{R}^{d \times \frac{d}{h}}$, where $h$ is the number of attention heads, and $\ell$ ranges from 1 to $h$.
    - Each attention head performs attention independently:
      - output $_{\ell}=\operatorname{softmax}\left(X Q_{\ell} K_{\ell}^{\top} X^{\top}\right) * X V_{\ell}$, where output $_{\ell} \in \mathbb{R}^{d / h}$
    - Then the outputs of all the heads are combined!
      - output $=Y$ [output~1~; ...; output~h~], where $Y \in \mathbb{R}^{d \times d}$
    - Each head gets to "look" at different things, and construct value vectors differently.
  - Same amount of computation as single-head selfattention!

### Training

- Residual connections [[He et al., 2016](https://arxiv.org/abs/1512.03385)]

  - Instead of $X^{(i)}=\operatorname{Layer}\left(X^{(i-1)}\right)$ (where $i$ represents the layer)
  - We let $X^{(i)}=X^{(i-1)}+\operatorname{Layer}\left(X^{(i-1)}\right)$ (so we only have to learn "the residual" from the previous layer)
  - Residual connections are thought to make the loss landscape considerably smoother (thus easier training!)
    - ​	<img src="image.assets/Screen Shot 2022-03-16 at 18.49.47.png" alt="Screen Shot 2022-03-16 at 18.49.47" style="zoom: 25%;" />
    - [Loss landscape visualization, [Li et al., 2018](https://arxiv.org/pdf/1712.09913.pdf), on a ResNet]

- Layer normalization [[Ba et al., 2016](https://arxiv.org/abs/1607.06450)]

  - Idea: cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviation within each layer.
  - LayerNorm’s success may be due to its normalizing gradients [[Xu et al., 2019](https://papers.nips.cc/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf)]

- Scaled dot product  [Vaswani et al., 2017]

  - When dimensionality 𝑑 becomes large, dot products between vectors tend to become large. 
    - Because of this, inputs to the softmax function can be large, making the gradients small.

  - We divide the attention scores by $\sqrt{d / h}$, to stop the scores from becoming large just as a function of $d / h$ (The dimensionality divided by the number of heads.)

  $$
  \text { output }_{\ell}=\operatorname{softmax}\left(\frac{x Q_{\ell} K_{\ell}^{\top} X^{\top}}{\sqrt{d / h}}\right) * X V_{\ell}
  $$

### The Transformer Encoder-Decoder [[Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf)]

- Encoder
  - ​	<img src="image.assets/Screen Shot 2022-03-16 at 18.56.03.png" alt="Screen Shot 2022-03-16 at 18.56.03" style="zoom:25%;" />
- Decoder
  - ​	<img src="image.assets/Screen Shot 2022-03-16 at 18.56.29.png" alt="Screen Shot 2022-03-16 at 18.56.29" style="zoom:25%;" />
  - Cross-Attention: similar to Attention in seq2seq
    - Let $\mathrm{H}=\left[h_{1} ; \ldots ; h_{T}\right] \in \mathbb{R}^{T \times d}$ be the concatenation of <u>encoder</u> vectors.
    - Let $\mathrm{Z}=\left[z_{1} ; \ldots ; z_{T}\right] \in \mathbb{R}^{T \times d}$ be the concatenation of <u>decoder</u> vectors.
    - The output is defined as output $=\operatorname{softmax}\left(Z Q(H K)^{\top}\right) \times H V$.

### Drawbacks

- Quadratic compute in self-attention (today): 
  - Computing all pairs of interactions means our computation grows quadratically with the sequence length
  - For recurrent models, it only grew linearly
  - Recent work
    -  Linformer [[Wang et al., 2020](https://arxiv.org/pdf/2006.04768.pdf)]
      - Key idea: map the sequence length dimension to a lowerdimensional space for values, keys
    - BigBird [[Zaheer et al., 2021](https://arxiv.org/pdf/2006.04768.pdf)]
      - Key idea: replace all-pairs interactions with a family of other interactions, like local windows, looking at everything, and random interactions.
- Position representations: 
  - Are simple absolute indices the best we can do to represent position? 
  - Recent work
    - Relative linear position attention [[Shaw et al., 2018](https://arxiv.org/abs/1803.02155)] 
    - Dependency syntax-based position [[Wang et al., 2019](https://arxiv.org/pdf/1909.00383.pdf)]

## *Thinking*

- Solving the problem of parallel computation and association between ephemeral elements in RNN by Self-Attention

  - The Transformer's separate decoder and encoder are more similar in structure to Affine and CNN, and the natural language sequence problem is added to the network through positional encoding.

  - - Due to the similarity in structure, Residual and Batch Normalization in CNN are both beneficial to the training of Transformer.



- 通过Self-Attention解决RNN中并行计算和历时元素间关联的问题
  - Transformer单独的decoder和encoder在结构上其实和Affine以及CNN更加相似了，自然语言的顺序问题通过位置编码加入到网络中
    - 由于结构的相似，CNN中的Residual和Batch Normalization都有利于Transformer的训练



































