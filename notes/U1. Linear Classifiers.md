# **<u>Reading 1: Image Classification</u>** (Lecture 2)

## **Image Classification Problem**



### Semantic Gap

- An image is a tensor of integers between [0, 255]

### Challenges

- viewpoint variation

- illumination

- deformation

- occlusion

- background clutter

- intra-class variation

A good image classification model must be 

- invariant to the cross product of all these variations, while simultaneously 
- retaining sensitivity to the inter-class variations.

## **Data-Driven Approach**

### The image classification pipeline

- Input
- Learning
- Evaluation

## Nearest Neighbor Classifier

- Example image classification dataset: CIFAR-10

### L1 (Manhattan) Distance

- $d_{1}\left(I_{1}, I_{2}\right)=\sum_{p}\left|I_{1}^{p}-I_{2}^{p}\right|$
  given two images and representing them as vectors $I_1,I_2$

### L2 (Eucidean) Distance

- $d_{2}\left(I_{1}, I_{2}\right)=\sqrt{\sum_{p}\left(I_{1}^{p}-I_{2}^{p}\right)^{2}}$
  - in a practical nearest neighbor application we could leave out the square root operation because square root is a *monotonic function*.

### L1 vs. L2

- In particular, the L2 distance is much more unforgiving than the L1 distance when it comes to differences between two vectors. 
- That is, the L2 distance prefers many medium disagreements to one big one. 

## k - Nearest Neighbor Classifier

- Instead of finding the single closest image in the training set, we will find the top **k** closest images, and have them vote on the label of the test image.
- Intuitively, higher values of **k** have a smoothing effect that makes the classifier more resistant to outliers.

### Validation sets for Hyperparameter tuning

- *Evaluate on the test set only a single time, at the very end.*

### Cross-validation

- ​	<img src="https://cs231n.github.io/assets/crossval.jpeg" alt="img" style="zoom:33%;" />
- instead of arbitrarily picking the first 1000 datapoints to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of *k* works by iterating over different validation sets and averaging the performance across these

### In practice

- to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive
- The splits people tend to use is between 50%-90% of the training data for training and rest for validation
  - if the number of hyperparameters is large you may prefer to use bigger validation splits
  - If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation
    - Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.

### Pros and Cons of Nearest Neighbor classifier

- fast for training
- slow at prediction
  - The classifier must *remember* all of the training data and store it for future comparisons with the test data. This is space inefficient because datasets may easily be gigabytes in size.
  - Classifying a test image is expensive since it requires a comparison to all training images.

## *Thinking*

- The challenge of the image classification problem: complexity leads to no hard-coded algorithm is up to the task
  - --〉Data-driven approaches
    - K-nearest neighbor algorithm: fast in training (just copy the dataset), but high space and time complexity in prediction
    - --〉Linear classification



- 图像分类问题的挑战：复杂性导致没有硬编码的算法能够胜任
- --〉数据驱动的方法
  - k-近邻算法：训练时快速（只需复制数据集），但预测时空间和时间复杂度高
  - --〉线性分类

# **<u>Reading 2. Linear Classification</u>** (Lecture 2-3)

## **Linear Classification**

### Parametric Approach

- a **score function** that maps the raw data to class scores, and 
- a **loss function** that quantifies the agreement between the predicted scores and the ground truth labels.
- We will then cast this as an **optimization** problem in which we will minimize the loss function with respect to the parameters of the score function.

### Parameterized mapping from images to label scores

- Let’s assume a training dataset of images $x_{i} \in R^{D}$, each associated with a label $y_{i} .$ Here $i=1 \ldots N$ and $y_{i} \in 1 \ldots K$.
  - That is, we have **N** examples (each with a dimensionality **D**) and **K** distinct categories.
  - For example, in CIFAR-10 we have a training set of **N** = 50,000 images, each with **D** = 32 x 32 x 3 = 3072 pixels, and **K** = 10
- We will now define the score function $f: R^{D} \mapsto R^{K}$ that maps the raw image pixels to class scores

### Linear classifier

$$
f\left(x_{i}, W, b\right)=W x_{i}+b
$$

- The image $x_i$ has all of its pixels flattened out to a single column vector of shape [D x 1]. 
- The matrix **W** (of size [K x D]), and the vector **b** (of size [K x 1]) are the **parameters** of the function.
  - The parameters in **W** are often called the **weights**, and 
  - **b** is called the **bias vector** because it influences the output scores, but without interacting with the actual data $x_i$.
- *Foreshadowing: Convolutional Neural Networks will map image pixels to scores exactly as shown above, but the mapping ( f ) will be more complex and will contain more parameters.*

### Analogy of images as high-dimensional points

<img src="https://cs231n.github.io/assets/pixelspace.jpeg" alt="pixelspace" style="zoom:33%;" />

- Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space 
  - (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). 
  - Analogously, the entire dataset is a (labeled) set of points.
- Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space.
- As we saw above, every row of $W$ is a classifier for one of the classes. 
  - The geometric interpretation of these numbers is that as we change one of the rows of $W$, the corresponding line in the pixel space will rotate in different directions. 
- The biases $b$, on the other hand, allow our classifiers to translate the lines. 
  - In particular, note that without the bias terms, plugging in $x_i=0$ would always give score of zero regardless of the weights, so all lines would be forced to cross the origin.

### Interpretation of linear classifiers as template matching

<img src="image.assets/templates-20220210103608013.jpg" alt="templates" style="zoom: 50%;" />

- Another interpretation for the weights $W$ is that each row of $W$ corresponds to a *template* (or sometimes also called a *prototype*) for one of the classes. 
  - The score of each class for an image is then obtained by comparing each template with the image using an *inner product* (or *dot product*) one by one to find the one that “fits” best.

### Bias trick

<img src="image.assets/wb.jpeg" alt="wb" style="zoom: 33%;" />

- A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector $x_i$ with one additional dimension that always holds the constant $1$ - a default *bias dimension*. With the extra dimension, the new score function will simplify to a single matrix multiply:

$$
f\left(x_{i}, W\right)=W x_{i}
$$

## **Loss functions**

- We are going to measure our unhappiness with outcomes such as this one with a **loss function** (or sometimes also referred to as the **cost function** or the **objective**).

## Multiclass Support Vector Machine (SVM) loss

- The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$.

- The score function takes the pixels and computes the vector $f(x_i,W)$ of class scores, which we will abbreviate to $s$ (short for scores). For example, the score for the j-th class is the j-th element: $s_j=f(x_i,W)_j$. 

- The Multiclass SVM loss for the i-th example is then formalized as follows:
  $$
  L_{i}=\sum_{j \neq y_{i}} \max \left(0, s_{j}-s_{y_{i}}+\Delta\right)
  $$

### Hinge Loss

- the threshold at zero $max(0,−)$ function is often called the **hinge loss**. 
- the squared hinge loss SVM (or L2-SVM), which uses the form $max(0,−)^2$ that penalizes violated margins more strongly (quadratically instead of linearly). 
- The unsquared version is more standard, but in some datasets the squared hinge loss can work better.
  - This can be determined during <u>*cross-validation*</u>.

### Regularization

- if some parameters $W$ correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $\lambda W$ where $\lambda \gt 1$ will also give zero loss

- We wish to encode some preference for a certain set of weights **W** over others to remove this ambiguity.

- We can do so by extending the loss function with a **regularization penalty** $R(W)$.

- The most common regularization penalty is the squared **L2** norm that discourages large weights through an elementwise quadratic penalty over all parameters:
  $$
  R(W)=\sum_{k} \sum_{l} W_{k, l}^{2}
  $$
  Notice that the regularization function is not a function of the data, it is only based on the weights.

  - It turns out that including the L2 penalty leads to the appealing **max margin** property in SVMs 
    (See [CS229](http://cs229.stanford.edu/notes/cs229-notes3.pdf) lecture notes for full details if you are interested).
  - The most appealing property is that penalizing large weights tends to **improve generalization**, because it means that no input dimension can have a very large influence on the scores all by itself.
    - Note that biases do not have the same effect since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights $W$ but not the biases $b$.
      However, in practice this often turns out to have a negligible effect.
    - Lastly, note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, because this would only be possible in the pathological setting of $W=0$.

### the full Multiclass Support Vector Machine loss

- made up of two components: the **data loss** (which is the average loss LiLi over all examples) and the **regularization loss**.

$$
L=\underbrace{\frac{1}{N} \sum_{i} L_{i}}_{\text {data loss }}+\underbrace{\lambda R(W)}_{\text {regularization loss }} \\ \\
\text{Or expanding this out in its full form:} \\
L=\frac{1}{N} \sum_{i} \sum_{j \neq y_{i}}\left[\max \left(0, f\left(x_{i} ; W\right)_{j}-f\left(x_{i} ; W\right)_{y_{i}}+\Delta\right)\right]+\lambda \sum_{k} \sum_{l} W_{k, l}^{2}
$$
- Where $N$ is the number of training examples. 
- As you can see, we append the regularization penalty to the loss objective, weighted by a hyperparameter $\lambda$. 
  There is no simple way of setting this hyperparameter and it is usually determined by <u>*cross-validation*</u>.

### Practical Considerations

- Setting Delta
  - It turns out that this hyperparameter can safely be set to $\Delta=1.0$ in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff
    - see Reading for detail

## Softmax classifier

- We now interpret the scores $f\left(x_{i} ; W\right)=W x_{i}$ as the unnormalized log probabilities for each class and replace the *hinge loss* with a **cross-entropy loss** that has the form:
  $$
  L_{i}=-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}\right) \quad \text { or equivalently } \quad L_{i}=-f_{y_{i}}+\log \sum_{j} e^{f_{j}}
  $$

  - where we are using the notation $f_{j}$ to mean the j-th element of the vector of class scores $f$. 
  - As before, the full loss for the dataset is the mean of $L_{i}$ over all training examples together with a regularization term $R(W)$. 
  - The function $f_{j}(z)=\frac{e^{z_{j}}}{\sum_{k} e^{z_{k}}}$ is called the **softmax function**: It takes a vector of arbitrary real-valued scores (in $z$ ) and squashes it to a vector of values between zero and one that sum to one.
    - Interpretations: see notes for detail (information theory, probability)

### Practical issues: Numeric stability

- Dividing large numbers can be numerically unstable, so it is important to use a normalization trick.
  $$
  \frac{e^{f_{y_{i}}}}{\sum_{j} e^{f_{j}}}=\frac{C e^{f_{y_{i}}}}{C \sum_{j} e^{f_{j}}}=\frac{e^{f_{y_{i}}+\log C}}{\sum_{j} e^{f_{j}+\log C}}
  $$

  - the numerical stability of the computation. A common choice for $C$ is to set $\log C=-\max _{j} f_{j}$. This simply states that we should shift the values inside the vector $f$ so that the highest value is zero.
  - see notes for code

### Possibly confusing naming conventions

- To be precise, the ***SVM classifier*** uses the ***hinge loss***, or also sometimes called the ***max-margin loss***. 
- The ***Softmax classifier*** uses the ***cross-entropy loss***. The Softmax classifier gets its name from the ***softmax function***, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. 
  - In particular, note that technically it doesn’t make sense to talk about the “softmax loss”, since softmax is just the squashing function, but it is a relatively commonly used shorthand.

## SVM vs. Softmax

- The SVM interprets these as class scores and its loss function *<u>encourages the correct class to have a score higher by a margin than the other class scores</u>*. 
- The Softmax classifier instead interprets the scores as (unnormalized) log probabilities for each class and then *<u>encourages the (normalized) log probability of the correct class to be high</u>* (equivalently the negative of it to be low).

### Softmax classifier provides “probabilities” for each class

- The reason we put the word “probabilities” in quotes, however, is that how peaky or diffuse these probabilities are depends directly on the regularization strength $\lambda$ - which you are in charge of as input to the system.
  - If the regularization strength $\lambda$ was higher, the weights $W$ would be penalized more and this would lead to smaller weights and the probabilites are now more diffuse.
    Moreover, in the limit where the weights go towards tiny numbers due to very strong regularization strength $\lambda$, the output probabilities would be near uniform.

### In practice, SVM and Softmax are usually comparable

- The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.
  - The Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. 
  - However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint.

## **Summary**

- We defined a **score function** from image pixels to class scores (in this section, a linear function that depends on weights **W** and biases **b**).
- Unlike kNN classifier, the advantage of this **parametric approach** is that once we learn the parameters we can discard the training data. Additionally, the prediction for a new test image is fast since it requires a single matrix multiplication with **W**, not an exhaustive comparison to every single training example.
- We introduced the **bias trick**, which allows us to fold the bias vector into the weight matrix for convenience of only having to keep track of one parameter matrix.
- We defined a **loss function** (we introduced two commonly used losses for linear classifiers: the **SVM** and the **Softmax**) that measures how compatible a given set of parameters is with respect to the ground truth labels in the training dataset. We also saw that the loss function was defined in such way that making good predictions on the training data is equivalent to having a small loss.

### Further Reading

- [Deep Learning using Linear Support Vector Machines](https://arxiv.org/abs/1306.0239) from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.

## *Thinking*

- Linear classification

- Loss function: the measure of satisfaction with prediction results

- - Support vector machine SVM: hinge loss
  - Softmax classifier: cross-entropy loss



- 线性分类
- 损失函数：衡量对预测结果的满意程度
  - 支持向量机SVM：铰链损失
  - Softmax分类器：交叉熵损失

# **<u>Reading 3. Optimization</u>** (Lecture 3)

## **Optimization**

- **Optimization** is the process of finding the set of parameters $W$ that minimize the loss function.

### Non-differentiable loss functions

- However, the [subgradient](http://en.wikipedia.org/wiki/Subderivative) still exists and is commonly used instead. In this class will use the terms *subgradient* and *gradient* interchangeably.

### Optimization

- A first very bad idea solution

  - Strategy #1: Random search

- Core idea: iterative refinement

  - Strategy #2: Random Local Search

    - to extend one foot in a random direction and then take a step only if it leads downhill

  - Strategy #3: Following the Gradient

    - The direction will be related to the **gradient** of the loss function:
      $$
      \frac{d f(x)}{d x}=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}
      $$

### Computing the gradient

- A slow, approximate but easy way (**numerical gradient**)
- A fast, exact but more error-prone way that requires calculus (**analytic gradient**).

### Numerically with finite differences

- Practical considerations
	- it is often sufficient to use a very small value (such as 1e-5 as seen in the example).
	- it often works better to compute the numeric gradient using the **centered difference formula**:
    $$
    [f(x+h)-f(x-h)] / 2 h
    $$
  
- Update in negative gradient direction

- Effect of step size

- **A problem of efficiency**

  - Evaluating the numerical gradient has complexity linear in the number of parameters

#### Analytically with Calculus

- The second way to compute the gradient is analytically using Calculus, which allows us to derive a direct formula for the gradient (no approximations) that is also very fast to compute.
  - However, unlike the numerical gradient it can be more error prone to implement
  - In practice it is very common to compute the analytic gradient and compare it to the numerical gradient to check the correctness of your implementation. This is called a **gradient check**.

- Example: SVM Classifier / Hinge Loss
	- The example of the SVM loss function for a single datapoint:
    $$
    L_{i}=\sum_{j \neq y_{i}}\left[\max \left(0, w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta\right)\right]
    $$
	- Taking the gradient with respect to $w_{y_i}$ we obtain:
    $$
    \nabla_{w_{y_{i}}} L_{i}=-\left(\sum_{j \neq y_{i}} \mathbb{1}\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta>0\right)\right) x_{i}
    $$
    (when you’re implementing this in code you’d simply count the number of classes that didn’t meet the desired margin (and hence contributed to the loss function) and then the data vector $x_i$ scaled by this number is the gradient.)
	- For the other rows where $j\neq y_i$ the gradient is:
    $$
    \nabla_{w_{j}} L_{i}=\mathbb{1}\left(w_{j}^{T} x_{i}-w_{y_{i}}^{T} x_{i}+\Delta>0\right) x_{i}
    $$

### Gradient Descent

```python
# Vanilla Gradient Descent
while True:
	weights_grad = evaluate_gradient(loss_fun, data, weights)
  weights += - step_size * weights_grad # perform parameter update
```

### Mini-batch gradient descent

- It seems wasteful to compute the full loss function over the entire training set in order to perform only a single parameter update.
  - A very common approach to addressing this challenge is to compute the gradient over **batches** of the training data.

```python
# Vanilla Minibatch Gradient Descent
while True:
  data_batch = sample_training_data(data, 256) # sample 256 examples
  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  weights += - step_size * weights_grad # perform parameter update
```

- The extreme case of this is a setting where the mini-batch contains only a single example. 
  - This process is called **Stochastic Gradient Descent (SGD)** (or also sometimes **on-line** gradient descent).
  - In practice due to **vectorized code optimizations** it can be computationally much more efficient to evaluate the gradient for 100 examples, than the gradient for one example 100 times.
  - Even though SGD technically refers to using a single example at a time to evaluate the gradient, you will hear people use the term SGD even when referring to mini-batch gradient descent
- The size of the mini-batch is a hyperparameter but it is not very common to cross-validate it.
  - We use powers of 2 in practice because <u>*many vectorized operation implementations work faster when their inputs are sized in powers of 2*</u>.

## **Summary**

![dataflow](image.assets/dataflow.jpeg)

- Summary of the information flow.
  - The **dataset** of pairs of **(x,y)** is given and fixed.
  - The **weights** start out as random numbers and can change.
  - During the forward pass the **score function** computes class scores, stored in vector **f**.
  - The **loss function** contains two components:
    - The **data loss** computes the compatibility between the scores **f** and the labels **y**.
    - The **regularization loss** is only a function of the weights.
  - During **Gradient Descent**, we compute the gradient on the weights (and optionally on data if we wish) and use them to perform a parameter update during Gradient Descent.

- In this section,
  - We developed the intuition of the loss function as a **high-dimensional optimization landscape** in which we are trying to reach the bottom. The working analogy we developed was that of a blindfolded hiker who wishes to reach the bottom. In particular, we saw that the SVM cost function is piece-wise linear and bowl-shaped.
  - We motivated the idea of optimizing the loss function with **iterative refinement**, where we start with a random set of weights and refine them step by step until the loss is minimized.
  - We saw that the **gradient** of a function gives the steepest ascent direction and we discussed a simple but inefficient way of computing it numerically using the finite difference approximation (the finite difference being the value of *h* used in computing the numerical gradient).
  - We saw that the parameter update requires a tricky setting of the **step size** (or the **learning rate**) that must be set just right: if it is too low the progress is steady but slow. If it is too high the progress can be faster, but more risky. We will explore this tradeoff in much more detail in future sections.
  - We discussed the tradeoffs between computing the **numerical** and **analytic** gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a **gradient check**, in which its implementation is compared to the numerical gradient.
  - We introduced the **Gradient Descent** algorithm which iteratively computes the gradient and performs a parameter update in loop.

## *Thinking*

- Optimization: Optimize the weight vector according to the loss function
  
- - Gradient descent

- General steps of deep learning.

- - **Dataset** (x,y) pairs are given and fixed.
  - **Weights** start as random numbers and can be changed.
  - During the forward pass, the **score function** calculates the class score, which is stored in vector f

- The **loss function** contains two parts:

- - - **data loss** calculates the compatibility between the score f and the label y

  - **regularization loss** is just a function of the weights

  - During **gradient descent**, we compute the gradients of the weights (and the gradients of the data if we wish) and use them to perform parameter updates during gradient descent.



- 优化：根据损失函数优化权重向量
  - 梯度下降
- 深度学习的一般步骤：
  - (x,y)对**数据集**是给定和固定的
  - **权重**开始时是随机数，可以改变
  - 在前向传递过程中，**得分函数**计算类的分数，存储在向量f中
  - **损失函数**包含两个部分
    - **数据损失**计算分数f和标签y之间的兼容性
    - **正则化损失**只是权重的函数
  - 在**梯度下降**过程中，我们计算权重的梯度（如果我们愿意，还可以计算数据的梯度），并在梯度下降过程中使用它们来执行参数更新

# <u>**Reading 4: Backpropagation**</u> (Lecture 4)

## **Backpropagation**

- A way of computing gradients of expressions through recursive application of **chain rule**.

### Example

$$
f(x, y, z)=(x+y) z
$$

<img src="image.assets/Screen Shot 2022-02-10 at 17.41.56.png" alt="Screen Shot 2022-02-10 at 17.41.56" style="zoom: 25%;" />

- The **forward pass** computes values from inputs to output (shown in green). 
- The **backward pass** then performs backpropagation which starts at the end and recursively applies the chain rule to compute the gradients (shown in red) all the way to the inputs of the circuit. 
- The gradients can be thought of as flowing backwards through the circuit.

### Intuitive understanding of backpropagation

- Backpropagation can be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.

### Modularity: Sigmoid example

- see notes for detail
- The point of this section is that the details of how the backpropagation is performed, and which parts of the forward function we think of as gates, is a matter of convenience. It helps to be aware of which parts of the expression have easy local gradients, so that they can be chained together with the least amount of code and effort.

### Patterns in backward flow

- The **add gate** always takes the gradient on its output and distributes it equally to all of its inputs.
- The **max gate** routes the gradient.
- The **multiply gate**'s local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule.

### Gradients for vectorized operations

- One must pay closer attention to dimensions and transpose operations.
- **Work with small, explicit examples**. Some people may find it difficult at first to derive the gradient updates for some vectorized expressions. Our recommendation is to explicitly write out a minimal vectorized example, derive the gradient on paper and then generalize the pattern to its efficient, vectorized form.
- Erik Learned-Miller has also written up a longer related document on taking matrix/vector derivatives which you might find helpful. [Find it here](http://cs231n.stanford.edu/vecDerivs.pdf).

## **Summary**

- We developed intuition for what the gradients mean, how they flow backwards in the circuit, and how they communicate which part of the circuit should increase or decrease and with what force to make the final output higher.
- We discussed the importance of **staged computation** for practical implementations of backpropagation. You always want to break up your function into modules for which you can easily derive local gradients, and then chain them with chain rule. Crucially, you almost never want to write out these expressions on paper and differentiate them symbolically in full, because you never need an explicit mathematical equation for the gradient of the input variables. Hence, decompose your expressions into stages such that you can differentiate every stage independently (the stages will be matrix vector multiplies, or max operations, or sum operations, etc.) and then backprop through the variables one step at a time.

## *Thinking*

- Backward propagation algorithm
  - Compute gradients for each layer iteration using the chain rule (visualized by the computational graph)
  
- 反向传播算法
  - 使用链式法则对每层迭代计算梯度（通过computational graph直观化）















